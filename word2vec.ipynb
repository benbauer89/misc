{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## transform files from directory \n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.stop = set(nltk.corpus.stopwords.words('english'))\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield [i for i in unicode(line, 'utf-8').lower().split() if i not in self.stop]\n",
    " \n",
    "## file needs to have utf-8 encoding and each entity (question) must be a new line\n",
    "sentences = MySentences('C:/Users/bbauer/Desktop/Neuer Ordner') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train the model\n",
    "model = gensim.models.word2vec.Word2Vec(sentences, iter=10, min_count=4, window = 5, size=25, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know\n",
      "see\n",
      "like\n",
      "want\n",
      "love\n",
      "feel\n",
      "im\n",
      "\"hi\n",
      "get\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "# get the most common words\n",
    "\n",
    "for i in range(10):\n",
    "    print(model.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5286053245101119\n"
     ]
    }
   ],
   "source": [
    "# some similarity fun\n",
    "print(model.wv.similarity('married', 'partner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'true', 0.8646132946014404),\n",
       " (u'truly', 0.8112808465957642),\n",
       " (u'love.', 0.7119335532188416),\n",
       " (u'desire', 0.703091561794281),\n",
       " (u'forever', 0.691092848777771),\n",
       " (u'happiness', 0.6856699585914612),\n",
       " (u'real', 0.6750628352165222),\n",
       " (u'truely', 0.673657238483429),\n",
       " (u'soulmate', 0.6717501878738403),\n",
       " (u'partner,', 0.6608835458755493)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bills', 0.8869832754135132),\n",
       " (u'money.', 0.845816969871521),\n",
       " (u'debt', 0.8036943674087524),\n",
       " (u'property', 0.7937832474708557),\n",
       " (u'bills.', 0.7893637418746948),\n",
       " (u'$', 0.7865709066390991),\n",
       " (u'income', 0.7794181704521179),\n",
       " (u'cover', 0.7759976387023926),\n",
       " (u'rent', 0.7684590816497803),\n",
       " (u'debts', 0.7665688395500183)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'child', 0.8727279305458069),\n",
       " (u'father', 0.8460017442703247),\n",
       " (u'son', 0.8367814421653748),\n",
       " (u'mom', 0.821607768535614),\n",
       " (u'mother', 0.8206684589385986),\n",
       " (u'youngest', 0.8034934401512146),\n",
       " (u'dad', 0.802269697189331),\n",
       " (u'daughter', 0.7974161505699158),\n",
       " (u'kid', 0.7878149151802063),\n",
       " (u'kids', 0.7786372303962708)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"baby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'chakra', 0.8600382208824158),\n",
       " (u'chakras.', 0.8585511445999146),\n",
       " (u'guided', 0.7996877431869507),\n",
       " (u'angelic', 0.7888866066932678),\n",
       " (u'cleansing', 0.7875677347183228),\n",
       " (u'cord', 0.7859991192817688),\n",
       " (u'aura', 0.7838760614395142),\n",
       " (u'recovery?\"', 0.7739477157592773),\n",
       " (u'cleanse', 0.7633254528045654),\n",
       " (u'blockages', 0.7625334858894348)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"chakras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'carrer', 0.9075772166252136),\n",
       " (u'education', 0.8900696039199829),\n",
       " (u'finance', 0.8731331825256348),\n",
       " (u'studies', 0.8555986285209656),\n",
       " (u'sucessful', 0.8517412543296814),\n",
       " (u'academic', 0.8509454727172852),\n",
       " (u'accounting', 0.8422917127609253),\n",
       " (u'sector', 0.8409655094146729),\n",
       " (u'successful?\"', 0.8398696780204773),\n",
       " (u'cosmetology', 0.8387865424156189)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"carrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'work.', 0.8619329929351807),\n",
       " (u'working', 0.8003853559494019),\n",
       " (u'shift', 0.7973617315292358),\n",
       " (u'work,', 0.7732657194137573),\n",
       " (u'business', 0.768303632736206),\n",
       " (u'function', 0.7581888437271118),\n",
       " (u'part', 0.7326571941375732),\n",
       " (u'sort', 0.7282631993293762),\n",
       " (u'school', 0.7149479985237122),\n",
       " (u'discuss', 0.7033078670501709)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'xmas', 0.8376498818397522),\n",
       " (u'christmas.', 0.8252521753311157),\n",
       " (u'holidays', 0.8088715672492981),\n",
       " (u'evening', 0.7942546010017395),\n",
       " (u'saturday', 0.7884491086006165),\n",
       " (u'meal', 0.7874487042427063),\n",
       " (u'nye', 0.7728686928749084),\n",
       " (u'weekend', 0.7668398022651672),\n",
       " (u'sunday', 0.7583177089691162),\n",
       " (u'dinner', 0.747183620929718)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"christmas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "# which part doesn't fit\n",
    "print(model.wv.doesnt_match(\"i work for money and joy\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['this', 'is', 'a', 'hello', 'test', 'love'], [2164, 1535, 4])\n"
     ]
    }
   ],
   "source": [
    "# convert the input data into a list of integer indexes aligning with the wv indexes\n",
    "# Read the data into a list of strings.\n",
    "\n",
    "def convert_data_to_index(string_data, wv):\n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            index_data.append(wv.vocab[word].index)\n",
    "    return index_data\n",
    "\n",
    "str_data = ['this', 'is', 'a', 'hello', 'test', 'love']\n",
    "index_data = convert_data_to_index(str_data, model.wv)\n",
    "print(str_data[:], index_data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab['love'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(model.wv.vocab), 25)) # 25 = size of word vector \n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55069L, 25L)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.69550574,  0.09775967,  1.43685794,  1.31089389, -0.90071911,\n",
       "        0.43822721,  2.78542066, -3.53648949,  1.4707433 ,  1.79783607,\n",
       "       -1.0285027 , -2.02234173, -2.15695143,  0.40224507, -0.2398791 ,\n",
       "        1.23964202, -0.06145307,  1.47738457, -3.73910785, -0.49204144,\n",
       "        4.01834393,  2.59854293, -0.88686156,  1.72344446,  1.45810735])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.6955057 ,  0.09775967,  1.4368579 ,  1.3108939 , -0.9007191 ,\n",
       "        0.4382272 ,  2.7854207 , -3.5364895 ,  1.4707433 ,  1.7978361 ,\n",
       "       -1.0285027 , -2.0223417 , -2.1569514 ,  0.40224507, -0.2398791 ,\n",
       "        1.239642  , -0.06145307,  1.4773846 , -3.7391078 , -0.49204144,\n",
       "        4.018344  ,  2.598543  , -0.88686156,  1.7234445 ,  1.4581074 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text classification in Keras using pre trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#MAX_SEQUENCE_LENGTH = 1000\n",
    "#MAX_NUM_WORDS = 20000\n",
    "#EMBEDDING_DIM = 100\n",
    "#VALIDATION_SPLIT = 0.2\n",
    "\n",
    "## We will only consider the top 20,000 most commonly occuring words in the dataset, \n",
    "### and we will truncate the sequences to a maximum length of 1000 words.\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS) \n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=2, batch_size=128)\n",
    "## We can also test how well we would have performed by not using pre-trained word embeddings, \n",
    "# but instead initializing our Embedding layer from scratch and learning its weights during training. \n",
    "# We just need to replace our Embedding layer with the following:\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
